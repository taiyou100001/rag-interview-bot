# RAG, æ ¹æ“šè·æ¥­åç¨±æŸ¥æ‰¾è³‡æ–™
from llama_index.core.settings import Settings # å…¨åŸŸè¨­å®š
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import os

Settings.llm = None # å¼·åˆ¶é—œæ‰å…¨åŸŸçš„ LLM æ¨¡å‹

def build_search_index(data_dir="data"):
    embed_model = HuggingFaceEmbedding(
        model_name="nomic-ai/nomic-embed-text-v1",
        trust_remote_code=True
    )
    documents = SimpleDirectoryReader(data_dir).load_data()
    index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
    return index

def query_job_knowledge(index, query):
    query_engine = index.as_query_engine(llm=None)#"gpt-3.5-turbo", response_mode="tree_summarize")
    response = query_engine.query(query)
    return response

if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"  # é¿å… warning
    index = build_search_index()
    question = input("è«‹è¼¸å…¥è¦æŸ¥è©¢çš„è·ä½é—œéµå­—æˆ–å•é¡Œï¼ˆä¾‹å¦‚ï¼šè¡ŒéŠ·é¢è©¦æŠ€å·§ï¼‰ï¼š")
    result = query_job_knowledge(index, question)
    print("\nğŸ“š æ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼š\n")
    print(result.response)
