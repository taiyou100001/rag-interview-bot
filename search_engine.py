from llama_index.core import SimpleDirectoryReader
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
from pathlib import Path
import os
import random
import json

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv(dotenv_path=Path(__file__).resolve().parent / "bin" / ".env")
TOKEN = os.environ.get("TOKEN")
if not TOKEN:
    raise ValueError("âŒ æ‰¾ä¸åˆ° HuggingFace TOKENï¼Œè«‹ç¢ºèª .env æª”æ¡ˆè¨­å®šã€‚")

# è¨­å®š InferenceClient
client = InferenceClient(provider="fireworks-ai", api_key=TOKEN)

def load_questions(data_dir="data"):
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"âŒ è³‡æ–™ç›®éŒ„ {data_dir} ä¸å­˜åœ¨ã€‚")
    
    documents = SimpleDirectoryReader(
        input_dir=data_dir,
        required_exts=[".json"],
        recursive=True
    ).load_data()
    
    if not documents:
        raise ValueError(f"âŒ {data_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON æ–‡ä»¶ã€‚")
    
    all_questions = []
    for doc in documents:
        try:
            questions = json.loads(doc.text)
            if not isinstance(questions, list):
                print(f"âš ï¸ è³‡æ–™æ ¼å¼éŒ¯èª¤æ–¼ {doc.metadata['file_path']}ï¼šâš ï¸ JSON æ ¼å¼éŒ¯èª¤ï¼šä¸æ˜¯ listã€‚")
                continue
            all_questions.extend(questions)
        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æéŒ¯èª¤æ–¼æª”æ¡ˆ {doc.metadata['file_path']}ï¼š{str(e)}")
            continue
    
    if not all_questions:
        raise ValueError("âŒ ç„¡æœ‰æ•ˆå•é¡Œæ•¸æ“šã€‚")
    
    return all_questions

def load_filtered_questions(job_title):
    """
    æ ¹æ“šè·ä½åç¨±éæ¿¾è³‡æ–™é›†å…§çš„å•é¡Œã€‚è‹¥ç„¡ç¬¦åˆè·ä½ï¼Œå‰‡å›å‚³å…¨éƒ¨é¡Œç›®ã€‚
    """
    all_questions = load_questions()
    filtered = [q for q in all_questions if q.get("è·ä½", "") in job_title]

    if not filtered:
        print(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°èˆ‡ã€{job_title}ã€ç›¸é—œçš„é¡Œç›®ï¼Œå°‡ä½¿ç”¨æ‰€æœ‰é¡Œç›®ã€‚")
        return all_questions
    return filtered

def ask_next_question(questions, previous_answer=None, previous_question=None):
    if previous_answer and previous_question:
        prompt = f"ä½ æ˜¯ä¸€å€‹é¢è©¦å®˜ï¼Œæ ¹æ“šä»¥ä¸‹å•é¡Œå’Œå›ç­”ï¼Œç”Ÿæˆä¸€å€‹ç¹é«”ä¸­æ–‡ã€ç°¡æ½”ï¼ˆ20å­—ä»¥å…§ï¼‰ã€èˆ‡é¢è©¦ç›¸é—œçš„å®Œæ•´å•å¥ï¼Œé¿å…é™³è¿°å¥æˆ–å†—é•·å›æ‡‰ã€‚ä¾‹å¦‚ï¼š'å¦‚ä½•è™•ç†å·¥ä½œå£“åŠ›ï¼Ÿ'ï¼š\nå•é¡Œ: {previous_question}\nå›ç­”: {previous_answer}"
        try:
            completion = client.chat.completions.create(
                model="meta-llama/Llama-3.3-70B-Instruct",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ LLM æŸ¥è©¢å¤±æ•—ï¼š{str(e)}")
            return random.choice(questions)["é¡Œç›®"]
    
    return random.choice(questions)["é¡Œç›®"]

if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    try:
        # Example: Filter questions for a specific job title
        job_title = input("è«‹è¼¸å…¥è·ä½åç¨±ï¼ˆç•™ç©ºå‰‡ä½¿ç”¨æ‰€æœ‰é¡Œç›®ï¼‰ï¼š")
        if job_title:
            questions = load_filtered_questions(job_title)
        else:
            questions = load_questions()
        
        previous_answer = None
        previous_question = None
        
        while True:
            question = ask_next_question(questions, previous_answer, previous_question)
            print(f"\nğŸ“ å•é¡Œï¼š{question}")
            answer = input("ä½ çš„å›ç­”ï¼ˆè¼¸å…¥ 'é€€å‡º' çµæŸï¼‰ï¼š")
            if answer.lower() == "é€€å‡º":
                print("çµæŸé¢è©¦ç·´ç¿’ã€‚")
                break
            
            previous_question = question
            previous_answer = answer
            
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼š{str(e)}")