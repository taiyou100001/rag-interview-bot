from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
from pathlib import Path
import os
import random
import json

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv(dotenv_path=Path(__file__).resolve().parent / "bin" / ".env")
TOKEN = os.environ.get("TOKEN")
if not TOKEN:
    raise ValueError("âŒ æ‰¾ä¸åˆ° HuggingFace TOKENï¼Œè«‹ç¢ºèª .env æª”æ¡ˆè¨­å®šã€‚")

# è¨­å®š InferenceClient
client = InferenceClient(api_key=TOKEN)  # è‹¥ä½¿ç”¨ç¬¬ä¸‰æ–¹æä¾›è€…ï¼Œæ·»åŠ  provider="featherless-ai"

def load_questions(data_dir="data"):
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"âŒ è³‡æ–™ç›®éŒ„ {data_dir} ä¸å­˜åœ¨ã€‚")
    
    documents = SimpleDirectoryReader(
        input_dir=data_dir,
        required_exts=[".json"],
        recursive=True
    ).load_data()
    
    if not documents:
        raise ValueError(f"âŒ {data_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON æ–‡ä»¶ã€‚")
    
    all_questions = []
    for doc in documents:
        try:
            questions = json.loads(doc.text)
            all_questions.extend(questions)
        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æéŒ¯èª¤æ–¼æª”æ¡ˆ {doc.metadata['file_path']}ï¼š{str(e)}")
            continue
    
    if not all_questions:
        raise ValueError("âŒ ç„¡æœ‰æ•ˆå•é¡Œæ•¸æ“šã€‚")
    
    return all_questions

def ask_next_question(questions, previous_answer=None, previous_question=None):
    if previous_answer and previous_question:
        prompt = f"æ ¹æ“šä»¥ä¸‹å•é¡Œå’Œå›ç­”ï¼Œç”Ÿæˆä¸€å€‹ç›¸é—œçš„é¢è©¦å•é¡Œï¼ˆè‡ªç„¶èªè¨€ï¼‰:\nå•é¡Œ: {previous_question}\nå›ç­”: {previous_answer}"
        try:
            completion = client.chat.completions.create(
                model="HuggingFaceH4/zephyr-7b-beta",
                messages=[{"role": "user", "content": prompt}]
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ LLM æŸ¥è©¢å¤±æ•—ï¼š{str(e)}")
            return random.choice(questions)["é¡Œç›®"]
    
    return random.choice(questions)["é¡Œç›®"]

if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    try:
        questions = load_questions()
        previous_answer = None
        previous_question = None
        
        while True:
            question = ask_next_question(questions, previous_answer, previous_question)
            print(f"\nğŸ“ å•é¡Œï¼š{question}")
            answer = input("ä½ çš„å›ç­”ï¼ˆè¼¸å…¥ 'é€€å‡º' çµæŸï¼‰ï¼š")
            if answer.lower() == "é€€å‡º":
                print("çµæŸé¢è©¦ç·´ç¿’ã€‚")
                break
            
            previous_question = question
            previous_answer = answer
            
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼š{str(e)}")